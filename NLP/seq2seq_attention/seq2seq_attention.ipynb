{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、包导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "sys.version_info(major=3, minor=6, micro=10, releaselevel='final', serial=0)\n",
      "matplotlib 3.1.3\n",
      "numpy 1.16.2\n",
      "pandas 0.25.3\n",
      "sklearn 0.22.2.post1\n",
      "tensorflow 2.0.0\n",
      "tensorflow_core.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本步骤\n",
    "- 1. 预处理数据\n",
    "- 2. 建立模型\n",
    "- 2.1 encoder \n",
    "- 2.2 attention\n",
    "- 2.3 decoder\n",
    "- 3. evaluation \n",
    "- 3.1 given sentence, return translation results \n",
    "- 3.2 visualize results (attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### unicode -> ascii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May I borrow this book?\n",
      "¿Puedo tomar prestado este libro?\n"
     ]
    }
   ],
   "source": [
    "import unicodedata \n",
    "def unicode_to_ascii(s): # 处理了一下西班牙语(移除重音accent), 而且ascii好像大小比较小. \n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')  \n",
    "\n",
    "\n",
    "# 举了个例子:\n",
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "print(unicode_to_ascii(en_sentence))\n",
    "print(unicode_to_ascii(sp_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "def preprocess_sentence(w):\n",
    "    # 先变成 ASCII码. \n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # 标点符号前后加空格. \n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    # 多余的空格变成一个空格. \n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # 除了标点符号和字母外都替换为空格. \n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    \n",
    "    # 去掉前后空格, rstrip表示去掉尾部空格. strip表示去掉头部空格. \n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 文本中读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'en' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-328c2a4a8fff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0men_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0men\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'en' is not defined"
     ]
    }
   ],
   "source": [
    "data_path = './spa.txt'\n",
    "\n",
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples):\n",
    "    # 按行分割. \n",
    "    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    \n",
    "    # 形成句子对 [['英语', '西班牙语']] \n",
    "    sentence_pairs = [line.split('\\t') for line in lines]\n",
    "    \n",
    "    # 形成句子对 tuple [('英语', '西班牙语'), ('英语', '西班牙语')] \n",
    "    preprocessed_sentence_pairs = [\n",
    "        (preprocess_sentence(en), preprocess_sentence(sp)) for en, sp in sentence_pairs]\n",
    "    \n",
    "    # 对每一行中的 两个句子 做预处理操作. \n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "    \n",
    "    # 将[('good', 1), ('bad', -1), ('thx', 1)] 变为 ('good', 'bad', 'thx') 和 (1, -1, 1) 两个tuple类型\n",
    "    return zip(*word_pairs)\n",
    "\n",
    "en_dataset, sp_dataset = create_dataset(data_path, None)\n",
    "print(en[-1])\n",
    "print(sp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [(1, 2), (3, 4), (5, 6)]\n",
    "c, d = zip(*a)\n",
    "print(type(c))\n",
    "print(c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1, 2], [3, 4], [5, 6]]\n",
    "c, d = zip(*a)\n",
    "print(type(c))\n",
    "print(c, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 文本变数字序列. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 11\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(lang):\n",
    "    # num_words=None 代表对词表大小无要求.  filters='' 表示过滤也无要求. 按空格' '分割. \n",
    "    lang_tokenizer = keras.preprocessing.text.Tokenizer(\n",
    "        num_words=None, filters='', split=' ') \n",
    "    \n",
    "    # 拟合文本. \n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    \n",
    "    # 序列化后的文本. \n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    \n",
    "    # 对序列化后的文本作下padding. \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    # 返回序列化后的文本, 和tokenizer分割器. \n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "# 看下 tensor的最大长度. \n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "input_tensor, input_tokenizer = tokenizer(sp_dataset[0:30000])\n",
    "output_tensor, output_tokenizer = tokenizer(en_dataset[0:30000])\n",
    "\n",
    "# 实验. \n",
    "max_length_input = max_length(input_tensor) \n",
    "max_length_output = max_length(output_tensor)\n",
    "print(max_length_input, max_length_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 验证集和训练集的切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 24000, 6000, 6000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# 训练集和测试集切分 8:2\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(\n",
    "    input_tensor, output_tensor, test_size=0.2)\n",
    "\n",
    "# 显示长度\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,    8,    7, ...,    0,    0,    0],\n",
       "       [   1,    6,   43, ...,    0,    0,    0],\n",
       "       [   1,  775,   31, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   1,    7, 4167, ...,    0,    0,    0],\n",
       "       [   1, 3473,   31, ...,    0,    0,    0],\n",
       "       [   1,   12,  153, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tf.data.Dataset 创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(input_tensor, output_tensor, \n",
    "                 batch_size, epochs, shuffle):\n",
    "    \n",
    "    # 用了tf.data.Dataset. \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (input_tensor, output_tensor)) \n",
    "    \n",
    "    # 如果需要打乱的话\n",
    "    if shuffle: \n",
    "        dataset = dataset.shuffle(30000)\n",
    "    \n",
    "    \n",
    "    # repeat 数据重复了 epochs 份.     \n",
    "    # drop_remainder=True 代表. 如果大小不足一个batch_size, 丢弃. \n",
    "    dataset = dataset.repeat(epochs).batch(\n",
    "        batch_size, drop_remainder = True)\n",
    "    \n",
    "    return dataset \n",
    "\n",
    "batch_size = 64 \n",
    "epochs = 10 \n",
    "\n",
    "train_dataset = make_dataset(\n",
    "        input_tensor_train, target_tensor_train, batch_size, epochs, True)\n",
    "eval_dataset = make_dataset(\n",
    "        input_tensor_val, target_tensor_val, batch_size, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 16)\n",
      "(64, 11)\n"
     ]
    }
   ],
   "source": [
    "# 随便取一个看看 shape \n",
    "for x, y in train_dataset.take(1):\n",
    "    print(x.shape) \n",
    "    print(y.shape)\n",
    "    \n",
    "# [Batch_size, token_dim] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、模型建立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_units = 256 \n",
    "units = 1024 \n",
    "\n",
    "# tokenizer.word_index 是 word -> index 的字典. \n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1 \n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1、Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, encoding_units, batch_size):\n",
    "        # 这里的 encoding_units 是 Encoder 输出每个token 的dimension.  \n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoding_units = encoding_units\n",
    "        \n",
    "        # embedding需要 词表大小vocab_size 和 输出维度embedding_dim \n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # encoding_units代表      gru的time step. \n",
    "        # return_sequences=True代表      每步都输出output.  \n",
    "        # return state=True代表          每步都输出hidden_state. \n",
    "        self.gru = keras.layers.GRU(self.encoding_units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.encoding_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 16)\n",
      "sample_output.shape  (64, 16, 1024)\n",
      "sample_hidden.shape  (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(input_vocab_size, embedding_units, \n",
    "                 units, batch_size)\n",
    "\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(x, sample_hidden)  # 这里的 x 是之前采的一个样本. \n",
    "\n",
    "print(x.shape)\n",
    "print(\"sample_output.shape \", sample_output.shape)        # sample_output.shape = (64, 16, 1024) 这里的16代表一句话中有16个词.  \n",
    "print(\"sample_hidden.shape \", sample_hidden.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2、BahdanauAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):                  # 注意: 这里的units和下面call中的units是不同的，下面的是seq2seq的输出维度. \n",
    "                                                # 这里的units 只是 Dense后的units(好像可以随便设?) \n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)  # 和 encoder_outputs相乘. \n",
    "        self.W2 = tf.keras.layers.Dense(units)  # 和 decoder_hidden相乘. \n",
    "        self.V = tf.keras.layers.Dense(1)       # 和 tanh(W1*encoder_outputs + W2*decoder_hidden)相乘. \n",
    "\n",
    "        \n",
    "    # attention中的一个输入是 decoder中上一个timestep传来的decoder_hidden. \n",
    "    # attention中的另一个输入是 encoder中所有timestep的输出outputs. \n",
    "    def call(self, decoder_hidden, encoder_outputs):\n",
    "        # decoder_hidden.shape:  (batch_size, units) 需要拓展. \n",
    "        # encoder_hidden.shape:  (batch_size, length, units) \n",
    "        decoder_hidden_with_time_axis = tf.expand_dims(\n",
    "            decoder_hidden, 1)\n",
    "        \n",
    "        # before V: (batch_size, length, units)\n",
    "        # after  V: (batch_size, length, 1)   得到score. 每个timestep 一个 score.\n",
    "        score = self.V(\n",
    "            tf.nn.tanh(\n",
    "                self.W1(encoder_outputs) + self.W2(decoder_hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights.shape :  (batch_size, length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector.shape :     (batch_size, length, units)\n",
    "        # 计算: (batch_size, length, 1) * (batch_size, length, units) 只有最后一维对不上, tf 自动给做了padding.  \n",
    "        context_vector = attention_weights * encoder_outputs \n",
    "        \n",
    "        # context_vector.shape :     (batch_size, units)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_model = BahdanauAttention(units = 10)\n",
    "attention_results, attention_weights = attention_model(\n",
    "    sample_hidden, sample_output)\n",
    "\n",
    "# attention_results 是 context_vector. \n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_results.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3、decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_units, \n",
    "                 decoding_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.decoding_units = decoding_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n",
    "        self.gru = keras.layers.GRU(self.decoding_units,          # decoder 每一time step的输出维度.  \n",
    "                                    return_sequences=True,       # 同样需要每步都输出. \n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "        # 全连接层. \n",
    "        self.fc = keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.decoding_units)\n",
    "\n",
    "    def call(self, x, hidden, encoding_outputs):\n",
    "        # x.shape: (Batchsize, 1)\n",
    "        \n",
    "        # context_vector.shape:  (batch_size, units)\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            hidden, encoding_outputs)\n",
    "\n",
    "        # before embedding:       x.shape: (batch_size, 1)\n",
    "        # after embedding:        x.shape: (batch_size, 1, embedding_units) \n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # combined_x.shape:      (batch_size, 1, encoding_units + embedding_units)\n",
    "        combined_x = tf.concat(\n",
    "            [tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # output.shape:  (batch_size, 1, decoding_units)            疑问？为什么不是(batch_size, length, decoding_units)呢? \n",
    "        # state.shape :  (batch_size, decoding_units) \n",
    "        output, state = self.gru(combined_x)\n",
    "\n",
    "        # output.shape : (batch_size , hidden_size) 去掉了shape中的1. \n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output.shape == (batch_size, vocab)  返回的是一个词. \n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_output.shape:  (64, 4935)\n",
      "decoder_hidden.shape:  (64, 1024)\n",
      "decoder_attention_weights.shape:  (64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(output_vocab_size, embedding_units,\n",
    "                 units, batch_size)\n",
    "\n",
    "outputs = decoder(tf.random.uniform((batch_size, 1)),\n",
    "                 sample_hidden,\n",
    "                 sample_output)\n",
    "\n",
    "decoder_output, decoder_hidden, decoder_aw = outputs\n",
    "# 输出的是某一时刻的单词,  4935是词表大小\n",
    "print(\"decoder_output.shape: \", decoder_output.shape)\n",
    "print(\"decoder_hidden.shape: \", decoder_hidden.shape)\n",
    "print(\"decoder_attention_weights.shape: \", decoder_aw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "# from_logits 意思是 输出未经 softmax层,  reduction = 'none' 意思是不求取平均值\n",
    "loss_object = keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits = True, reduction = 'none')\n",
    "\n",
    "# 这里只是单步（time_step）的损失函数。 \n",
    "def loss_function(real, pred):\n",
    "    # 这里的mask是为了遮挡. real中padding的部分, 因为padding部分不放在损失函数中\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    # before : mask元素是True or False. \n",
    "    # after  : mask元素是0 or 1 \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask \n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、训练函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 单步损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加tf.function 可以做加速. \n",
    "@tf.function\n",
    "def train_step(inp, targ, encoding_hidden):\n",
    "    loss = 0 \n",
    "    with tf.GradientTape() as tape:\n",
    "        encoding_outputs, encoding_hidden = encoder(\n",
    "            inp, encoding_hidden)\n",
    "        \n",
    "        # decoding 的初始 hidden 是 encoder的最后一个 hidden. \n",
    "        decoding_hidden = encoding_hidden \n",
    "        \n",
    "        # eg: <start> I am here <end>\n",
    "        # 1. <start> -> I\n",
    "        # 2. I -> am \n",
    "        # 3. am -> here\n",
    "        # 4. here -> <end>\n",
    "        \n",
    "        # decoding 有 (句长 - 1) 步. \n",
    "        for t in range(0, targ.shape[1] - 1):\n",
    "            decoding_input = tf.expand_dims(targ[:, t], 1)\n",
    "            \n",
    "            predictions, decoding_hidden, _ = decoder(\n",
    "                decoding_input, decoding_hidden, encoding_outputs)\n",
    "            loss += loss_function(targ[:, t+1], predictions) \n",
    "     \n",
    "    \n",
    "    batch_loss = loss / int(targ.shape[0]) \n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables \n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.8139\n",
      "Epoch 1 Batch 100 Loss 0.3747\n",
      "Epoch 1 Batch 200 Loss 0.3240\n",
      "Epoch 1 Batch 300 Loss 0.3163\n",
      "Epoch 1 Batch 400 Loss 0.2600\n",
      "Epoch 1 Loss 0.3367\n",
      "Time take for 1 epoch 83.3097403049469 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.2471\n",
      "Epoch 2 Batch 100 Loss 0.2328\n",
      "Epoch 2 Batch 200 Loss 0.2399\n",
      "Epoch 2 Batch 300 Loss 0.2141\n",
      "Epoch 2 Batch 400 Loss 0.1453\n",
      "Epoch 2 Loss 0.2135\n",
      "Time take for 1 epoch 69.20133399963379 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.1588\n",
      "Epoch 3 Batch 100 Loss 0.1464\n",
      "Epoch 3 Batch 200 Loss 0.1352\n",
      "Epoch 3 Batch 300 Loss 0.1215\n",
      "Epoch 3 Batch 400 Loss 0.1029\n",
      "Epoch 3 Loss 0.1317\n",
      "Time take for 1 epoch 68.0903377532959 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.0821\n",
      "Epoch 4 Batch 100 Loss 0.0914\n",
      "Epoch 4 Batch 200 Loss 0.0929\n",
      "Epoch 4 Batch 300 Loss 0.0775\n",
      "Epoch 4 Batch 400 Loss 0.0492\n",
      "Epoch 4 Loss 0.0817\n",
      "Time take for 1 epoch 67.2599892616272 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0547\n",
      "Epoch 5 Batch 100 Loss 0.0542\n",
      "Epoch 5 Batch 200 Loss 0.0631\n",
      "Epoch 5 Batch 300 Loss 0.0622\n",
      "Epoch 5 Batch 400 Loss 0.0377\n",
      "Epoch 5 Loss 0.0525\n",
      "Time take for 1 epoch 67.55320429801941 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0258\n",
      "Epoch 6 Batch 100 Loss 0.0365\n",
      "Epoch 6 Batch 200 Loss 0.0413\n",
      "Epoch 6 Batch 300 Loss 0.0335\n",
      "Epoch 6 Batch 400 Loss 0.0295\n",
      "Epoch 6 Loss 0.0347\n",
      "Time take for 1 epoch 68.04390668869019 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0235\n",
      "Epoch 7 Batch 100 Loss 0.0145\n",
      "Epoch 7 Batch 200 Loss 0.0284\n",
      "Epoch 7 Batch 300 Loss 0.0278\n",
      "Epoch 7 Batch 400 Loss 0.0191\n",
      "Epoch 7 Loss 0.0248\n",
      "Time take for 1 epoch 68.1934757232666 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0142\n",
      "Epoch 8 Batch 100 Loss 0.0200\n",
      "Epoch 8 Batch 200 Loss 0.0229\n",
      "Epoch 8 Batch 300 Loss 0.0221\n",
      "Epoch 8 Batch 400 Loss 0.0075\n",
      "Epoch 8 Loss 0.0189\n",
      "Time take for 1 epoch 67.84542465209961 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0171\n",
      "Epoch 9 Batch 100 Loss 0.0126\n",
      "Epoch 9 Batch 200 Loss 0.0184\n",
      "Epoch 9 Batch 300 Loss 0.0159\n",
      "Epoch 9 Batch 400 Loss 0.0131\n",
      "Epoch 9 Loss 0.0151\n",
      "Time take for 1 epoch 67.87533974647522 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0139\n",
      "Epoch 10 Batch 100 Loss 0.0122\n",
      "Epoch 10 Batch 200 Loss 0.0108\n",
      "Epoch 10 Batch 300 Loss 0.0191\n",
      "Epoch 10 Batch 400 Loss 0.0080\n",
      "Epoch 10 Loss 0.0132\n",
      "Time take for 1 epoch 68.20220398902893 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# epochs 是 整个数据集遍历 次数. \n",
    "# step_per_epoch. 是每个epoch 需要遍历 batch的个数\n",
    "epochs = 10 \n",
    "steps_per_epoch = len(input_tensor) // batch_size \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time() \n",
    "    \n",
    "    encoding_hidden = encoder.initialize_hidden_state() \n",
    "    total_loss = 0 \n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(\n",
    "        train_dataset.take(steps_per_epoch)):\n",
    "        \n",
    "        batch_loss = train_step(inp, targ, encoding_hidden)\n",
    "        total_loss += batch_loss \n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                epoch + 1, batch, batch_loss.numpy()))\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                       total_loss / steps_per_epoch))\n",
    "    print('Time take for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 模型的预测. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单个句子的评估，如果batch个句子，需要修改 下面的 encoding_hidden \n",
    "def evaluate(input_sentence):\n",
    "    # 注意力矩阵, 观察输入和输出之间的关系. \n",
    "    attention_matrix = np.zeros((max_length_output, max_length_input))\n",
    "    # 预处理下句子. \n",
    "    input_sentence = preprocess_sentence(input_sentence)     \n",
    "    \n",
    "    \n",
    "    inputs = [input_tokenizer.word_index[token] \n",
    "              for token in input_sentence.split(' ')]\n",
    "    inputs = keras.preprocessing.sequence.pad_sequences(\n",
    "            [inputs], maxlen = max_length_input, padding = 'post') \n",
    "    # 转为tf.tensor类型\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    \n",
    "    results = '' \n",
    "    # encoding_hidden = encoder.initialize_hidden_state() \n",
    "    encoding_hidden = tf.zeros((1, units))\n",
    "    \n",
    "    encoding_outputs, encoding_hidden = encoder(inputs, encoding_hidden)\n",
    "    decoding_hidden = encoding_hidden \n",
    "    \n",
    "    # eg: <start> -> A\n",
    "    # A -> B -> C -> D\n",
    "    \n",
    "    # decoding_input.shape: (1, 1)\n",
    "    decoding_input = tf.expand_dims(\n",
    "        [output_tokenizer.word_index['<start>']], 0)\n",
    "    \n",
    "    for t in range(max_length_output):\n",
    "        predictions, decoding_hidden, attention_weights = decoder(\n",
    "            decoding_input, decoding_hidden, encoding_outputs)\n",
    "        \n",
    "        # attention_weights.shape: (batch_size, input_length, 1) -> (1, 16, 1) \n",
    "        attention_weights = tf.reshape(attention_weights, (-1,))\n",
    "        attention_matrix[t] = attention_weights.numpy()\n",
    "        \n",
    "        # predictions.shape: (batch_size, vocab_size) (1, 4935)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        results += output_tokenizer.index_word[predicted_id] + ' '\n",
    "        \n",
    "        if output_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return results, input_sentence, attention_matrix \n",
    "        \n",
    "        decoding_input = tf.expand_dims([predicted_id], 0)\n",
    "    return results, input_sentence, attention_matrix     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention_matrix, input_sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    # cmap='viridis' 不同值 显示 不同颜色\n",
    "    ax.matshow(attention_matrix, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "    # 最前面弄了个[''] 表示, 从第一个格子开始. \n",
    "    ax.set_xticklabels([''] + input_sentence, \n",
    "                       fontdict = fontdict, rotation = 90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def translate(input_sentence): \n",
    "    results, input_sentence, attention_matrix = evaluate(input_sentence)\n",
    "    \n",
    "    print(\"Input: %s\" % (input_sentence))\n",
    "    print(\"Predicted translation: %s\" % (results))\n",
    "    \n",
    "    attention_matrix = attention_matrix[:len(results.split(' ')),\n",
    "                                        :len(input_sentence.split(' '))]\n",
    "    plot_attention(attention_matrix, input_sentence.split(' '),\n",
    "                   results.split(' '))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> hace mucho frio aqui . <end>\n",
      "Predicted translation: it s very cold here . <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'Hace mucho frío aquí.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.0",
   "language": "python",
   "name": "tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
