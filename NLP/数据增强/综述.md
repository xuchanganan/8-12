# 运用文本增强技术的常见场景
- <font color='red'>少样本场景</font>  
运用文本增强技术来扩充样本集, 是一件又快又省, 性价比很高的事。  
- <font color='red'>分类任务中样本分布不均衡场景</font>  
**模型对于小样本类别往往处于欠拟合状态, 实际预测时, 几乎不会对这类别给予太高的概率。**  
针对小样本类别, 运用数据增强技术进行样本扩充, 从而降低样本间的不均衡性。  
相应论文：Chawla, Nitesh V., et al. "SMOTE: synthetic minority over-sampling technique." Journal of artificial intelligence research16 (2002): 321-357. ]
- <font color='red'>半监督训练场景</font>  
UDA：文本增强技术可以用在无标签样本数， 构造出半监督训练所需的样本对.  
很有趣的事：文本分类问题, 构造句子对. <"这是正面情感么?" , "文本内容"> 输出为 True or False.  
- <font color='red'>提高模型鲁棒性</font>  
**保持语义不变的情况下, 变换文本的表达形式**， 例如： 回译、文本复述  
**按照某种策略对原文局部调整**, 例如: 同义词替换、随机删除  
**可以使得模型更关注文本的语义信息，对文本局部噪声不敏感**，比如：  
文本数据强增技术帮助可以模型对于噪声局部不再感敏」，如果你依然能够看明白这句话的意思，说明你对于文本局部噪声也是不敏感的。  

# 回译  
**如果采用翻译模型，可以采用 random sample 或 beam search 等策略实现成倍数的数据扩充。如果采用 google 等翻译工具，通过更换中间语种，也可以实现 N 倍的数据扩充。**  
  
**目前翻译模型对长文本输入的支持较弱，因此在实际中，一般会将文本按照「。」等标点符号拆分为一条条句子，然后分别进行回译操作，最后再组装为新的文本。**  

<font color=red>用于NLP任务的半监督学习算法UDA, 仅仅用了20条样本作为标签数据, 就在IMDB数据集上实现了接近SOTA的性能。</font>  
- 回译的有效性来源于迁移学习， 将模型学到的知识转移到了新生成的样本上  
- 回译新样本有效性，隐含着一个先验，即模型对于具有不同语言表达形式但同样语义的输入文本，应该具有不变性，或者应该具有相近的输出  
<font color='red'>这样看， 数据增强扩充样本集， 和单纯在原样本上跑两个epoch是有很大区别的。</font> 

# 随机词替换
EDA(Easy data augmentation)的文本增强方法： 同义词替换、 随机插入、 随机交换和随机删除。  
- <font color='red'>同义词替换 (SR)：</font> 从句子中随机选择**非停止词**。用随机选择的同义词替换这些单词；
- <font color='red'>随机插入 (RI)：</font> 随机找出句中某个**非停用词**，并求出其随机的同义词，将该同义词插入句子的一个随机位置。重复 n 次；
- <font color='red'>随机交换 (Random Swap, RS)：</font> 随机的选择句中两个单词并交换它们的位置。重复 n 次；
- <font color='red'>随机删除 (RD)：</font> 以概率 p 随机删除句子中每个单词。  

在运用EDA技术，如何设置替换比例和增强的文本倍数, 比如2000条语句对应多少数据进行随机删除，增加。  
| 样本数 | 替换删除的比例 | 从每个句子扩展出的句子数量 |
| :-----| ----: | :----: |
| 500 | 0.05 | 16 |
| 2000 | 0.05 | 8 |
| 5000 | 0.1  | 4 |
| more | 0.1 | 4 |

比如： **同义词替换中, 替换的单词数 n = alpha * L, L 是 句子长度。**  

# 非核心词替换
