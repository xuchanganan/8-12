# Abstract  
EDA: easy augmentation techniques for boosting performance on text classification tasks.  
EDA包括:  
- 同义词替换.  
- 随机插入.  
- 随机替换.  
- 随机删除.  
  
论文证明:EDA在卷积和循环神经网络上有好的效果提升, 适应于smaller数据集. 只使用50%的数据就能实现之前全部数据得到的分数.  
在论文中也提到了建议的参数大小。 

# EDA  
&emsp;&emsp;对于在训练集上给定的句子, 我们随机地选择并且应用下面的操作之一:  
- 1. 同义词替换, 随机地从句子中选择**n个非停用词**, 对于每个词, 用这些词的同义词替换它们.  
- 2. 随机插入, 随即找一个非停用词的同义词,将这个同义词随机插入到句中某个位置, 这个操作做n次.  
- 3. 随机交换， 随机交换句子中两个字的位置, 这个操作做n次. 
- 4. 以概率p随机移除句中每个单词.  
  
因为长句子比短句子有更多的词, 他们在保证原始类别的时候可以吸收更多的噪声, 所以我们采用n = alpha * l, l是句子长度, alpha是比例. 对于每句话, 生成n_avg个增强的句子.  

# 实验步骤  
小数据集的大小.这里有: 7447， 4082， 9000， 5452， 39418; 从图中可以看到, 9000其实效果提升就不明显了。  
而且使用的model是RNN和CNN这种低容量的模型, 抗鲁棒性低(**思考：这是EDA在Bert上效果不好的原因吗？**)

