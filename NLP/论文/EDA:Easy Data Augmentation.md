# Abstract  
EDA: easy augmentation techniques for boosting performance on text classification tasks.  
EDA包括:  
- 同义词替换.  
- 随机插入.  
- 随机替换.  
- 随机删除.  
  
论文证明:EDA在卷积和循环神经网络上有好的效果提升, 适应于smaller数据集. 只使用50%的数据就能实现之前全部数据得到的分数.  
在论文中也提到了建议的参数大小。 

# EDA  
&emsp;&emsp;对于在训练集上给定的句子, 我们随机地选择并且应用下面的操作之一:  
- 1. 同义词替换, 随机地从句子中选择**n个非停用词**, 对于每个词, 用这些词的同义词替换它们.  
- 2. 随机插入, 随即找一个非停用词的同义词,将这个同义词随机插入到句中某个位置, 这个操作做n次.  
- 3. 随机交换， 随机交换句子中两个字的位置, 这个操作做n次. 
- 4. 以概率p随机移除句中每个单词.  
  
因为长句子比短句子有更多的词, 他们在保证原始类别的时候可以吸收更多的噪声, 所以我们采用n = alpha * l, l是句子长度, alpha是比例. 对于每句话, 生成n_avg个增强的句子.  

# 实验步骤  
&emsp;&emsp;小数据集的大小.这里有: 7447， 4082， 9000， 5452， 39418; 从图中可以看到, 9000其实效果提升就不明显了。  
而且使用的model是RNN和CNN这种低容量的模型, 抗鲁棒性低(**思考：这是EDA在Bert上效果不好的原因吗？因为Bert本身预训练自带mask,和随机替换**)  

实验显示EDA在较小数据集上有很大的提高.  

### 4.1 Does EDA conserve true labels? 
&emsp;&emsp;首先, 我们在PC分类任务上不用数据增强,训练一个RNN, 然后,我们在测试集test_set上做数据增强,每条做了9个.然后将原始数据和增强数据一起喂入RNN网络中,然后在最后一层上提取输出.观察结果:发现sentences augmented with EDA保存了原始数据的标签.  

# 推荐的使用参数
| Ntrain | alpha | navg |
|------|------| -----|
| 500  | 0.05 | 16   |
| 2000 | 0.05 | 8 |
| 5000 | 0.1  | 4 |
| More | 0.1  | 4 |
