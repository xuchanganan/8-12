# Abstract  
EDA: easy augmentation techniques for boosting performance on **text classification tasks**.  
EDA包括:  
- 同义词替换.  
- 随机插入.  
- 随机替换.  
- 随机删除.  
  
论文证明:EDA在卷积和循环神经网络上有好的效果提升, 适应于smaller数据集. 只使用50%的数据就能实现之前全部数据得到的分数.  
在论文中也提到了建议的参数大小。 

# EDA  
&emsp;&emsp;对于在训练集上给定的句子, 我们随机地选择并且应用下面的操作之一:  
- 同义词替换, 随机地从句子中选择**n个非停用词**, 对于每个词, 用这些词的同义词替换它们.  
- 随机插入, 随即找一个非停用词的同义词,将这个同义词随机插入到句中某个位置, 这个操作做n次.  
- 随机交换， 随机交换句子中两个字的位置, 这个操作做n次. 
- 以概率p随机移除句中每个单词.  
  
因为长句子比短句子有更多的词, 他们在保证原始类别的时候可以吸收更多的噪声, 所以我们采用n = alpha * l, l是句子长度, alpha是比例. 对于每句话, 生成n_avg个增强的句子.  

# 实验步骤  
&emsp;&emsp;小数据集的大小.这里有: 7447， 4082， 9000， 5452， 39418; 从图中可以看到, 9000其实效果提升就不明显了。  
而且使用的model是RNN和CNN这种低容量的模型, 抗鲁棒性低(**思考：这是EDA在Bert上效果不好的原因吗？因为Bert本身预训练自带mask,和随机替换**)  

实验显示EDA在较小数据集上有很大的提高.  

### 4.1 Does EDA conserve true labels? 
&emsp;&emsp;首先, 我们在PC分类任务上不用数据增强,训练一个RNN, 然后,我们在测试集test_set上做数据增强,每条做了9个.然后将原始数据和增强数据一起喂入RNN网络中,然后在最后一层上提取输出.观察结果:发现sentences augmented with EDA保存了原始数据的标签.  

### 4.2 How much augmentation? 
&emsp;&emsp;这里有扩增数量的实验, **对于更小的训练集, 过拟合是常见的. 因此生成许多扩增数据会有不错的效果提升**但对于大数据集来说,并无效果。  
**问题:扩增的数据集会放到测试集中吗？**  
**感觉应该不会单纯放到测试集中,因为当时放到测试集中只是为了论证扩增数据标签未改变这件事，所以才做了那样的操作。**  

# EDA's Limitations  
&emsp;&emsp;对于上面说的五个任务, 在用到全部数据集的情况下, 只提升了小于1%的提升.  
&emsp;&emsp;**当使用预训练模型的时候,EDA可能不会有显著提升.(暗自开心, 的确是自己分析的那样，因为本身预训练使用了大量的数据，而且也做了mask和随机词替换)  
比如BERT、ELMo、ULMFit...**

# 推荐的使用参数
| Ntrain | alpha | navg |
|------|------| -----|
| 500  | 0.05 | 16   |
| 2000 | 0.05 | 8 |
| 5000 | 0.1  | 4 |
| More | 0.1  | 4 |

# Implementation Deatil  
[代码见](http://github.com/jasonwei20/eda_nlp)
- Synonym thesaurus(同义词辞典): 同义词替换和随机插入, 是使用WordNet生成的.  
使用了WordNet作为同义词字典, 它是容易下载的.  

# 常见问题:
- **为什么使用EDA,而不是其他技术**, 比如说:contextual augmentation, 噪声, GAN 或者是回译.  
&emsp;&emsp;**作者鼓励尝试别的, 因为它们实际上可能比EDA更有效**, 但是因为这些技术需要深度学习模型的使用去生成扩增语句, 这样实现这些技术可能会带来高的代价, 而EDA的意向是提供简单的技术  
- 使用EDA是否可能损害已有模型的表现?  
可能会损害,但即便这样, “deep learning is robust to massive label noise”  
- 为什么EDA可以提升文本分类的效果?  
首先:生成和原始数据相差不大的数据引入了一些程度的噪声，这样可以防止过拟合;  
其次:使用EDA,引入了同义词, 使得模型可能在训练集中引入一些本来不再训练集中的，但在测试集中的词汇;  

- 直觉上随机交换、插入或者删除没有意义:  
其实只是为了尽可能不影响文本信息, 相当于只是在部分词和位置上增加了一些噪声. 这样对于防止过拟合其实有很大帮助.  
