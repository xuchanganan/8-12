&emsp;&emsp;Capsule: 提出了一种新的"vector in vector out"的传递方案, 并且这种方案在很大程度上是可解释的.  

底层的胶囊和高层的胶囊构成一些连接关系  

# 什么是胶囊? 
&emsp;&emsp;**只要把一个向量当作一个整体来看，它就是一个“胶囊”，是的，你没看错，你可以这样理解：神经元就是标量，胶囊就是向量**，
&emsp;&emsp;每一个胶囊表示一个属性，而胶囊的向量则表示这个属性的“标架”。也就是说，我们以前只是用一个标量表示有没有这个特征（比如有没有羽毛），现在我们用一个向量来表示，不仅仅表示有没有，还表示“有什么样的”（比如有什么颜色、什么纹理的羽毛），如果这样理解，就是说在对单个特征的表达上更丰富了。  

squash函数: 希望胶囊的模长能够代表这个特征的概率, squash将模长压缩到0-1里了.  

&emsp;&emsp;(**但是现在有个问题,现在要求的是上层胶囊,但是求上层胶囊还需要拿自己和下层进行内积, 可是求上层的时候并不知道上层是什么？自己:出现这个问题, 就像是EM的不知道隐变量I是什么,但是我还是要用,那很自然的就想到了赋初值,迭代,只要收敛.)**   
更好的解释看苏神的两个小菜, 胶囊实际上做的就是想办法将n个向量整合成一个向量x.  

# 动态路由.  
其实就是迭代过程, 先将内积<ui, vi>初始化为0.然后逐步迭代.   

# 全连接. 
&emsp;&emsp;如果下层胶囊后不紧跟FC层,那么每个上层胶囊都是以相同初值迭代, 这样会导致上层胶囊都相同.  
&emsp;&emsp;所以,为了从不同角度看输入,使得上层胶囊显然是不一样的.那么考虑下最简单的FC层连接(**自己:bert的多头注意力好像也是通过FC_embedding弄的..没有用不同卷积核**)  
&emsp;&emsp;**那选用FC提取不同特征,还是拿CNN.区别主要在于:FC输入大小时固定的，但是CNN的输入大小是可变的。因此全连接层的Capsule不能使用.**  

**(自己, 然后弄了个权值共享..固定上层胶囊j, 因为输出是固定的, 理解为卷积核吧, 核是给定的. 然后用每个核,这里是FC去与每个下层胶囊做交互.)**  

# 作用
**Capsule具有良好的整合特征的能力,  可以用来代替模型中的pooling层**. 

# 思考
(**自己:某个特征是其他所有特征的聚合,是根据相似度的聚合,那胶囊的连接, 难道不像attention吗?**)  
