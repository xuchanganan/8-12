# Abstract 
&emsp;&emsp;强调了一波GPT应用广泛性.

# Introduction.
&emsp;&emsp;从无标签文本数据中利用more than word-level的信息是一项挑战, 主要有两个原因:
- 不知道以什么样的目标, 学习一个怎样的文本表示用于迁移是有效的.
- 将学得的表示用于目标任务没有一个共识.
&emsp;&emsp;在这篇文章中, 作者提出了解决上述问题的一个尝试: 无监督学习的预训练和有监督的微调相结合.首先, 我们使用一个语言模型的目标在无标签数据上学习模型的初始参数.然后再用特定任务的目标学习.




