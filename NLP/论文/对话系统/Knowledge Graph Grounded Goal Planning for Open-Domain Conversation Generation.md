# Abstract
&emsp;&emsp;受到人-人对话策略的启发, 论文作者将开放领域的对话生成分为两个子任务:1、明确的目标序列规划; 2、通过主题细化完成目标.为此,论文作者提出了一个基于知识图谱的层次化强化学习框架（KnowHRL）
&emsp;&emsp;具体地,对于目标序列规划(也就是聊天主题)这一子任务, 上层策略学习通过遍历一个知识图谱去选择一个高层次的目标序列, 该目标序列(聊天主题)应该同时兼顾 对话连贯性 和 主题适合性.  
&emsp;&emsp;对于回复生成子任务,中层和底层策略相互结合, 利用目标驱动的生成机制生成一个更有深度的对话回复.  
&emsp;&emsp;这两种策略使得聊天机器人在开放领域上可以开展更加主动的对话.  

# Introduction
&emsp;&emsp;基于Seq2Seq的模型倾向于生成一般的、不连贯的回复.为了解决这个问题, 
- 之前的研究尝试引入**额外的知识或主题信息**来提高对话信息量;
- 也有一些研究者以生成连贯的、长期的多轮对话为目标，进行强化训练;
&emsp;&emsp;尽管上述模型已实现了明显的结果,然而他们生成的质量依旧不高，尤其是对于一个长对话.  

&emsp;&emsp;最上层的层次策略是MLP模型, 用来在知识图谱上学习一条**结点**路径,为了计划一条连贯的,多方向的,长的目标序列, 同时还会考虑用户反馈的信息(比如新的主题),为了和用户兴趣更加匹配.  
&emsp;&emsp;中层的层次策略是基于MLP的另一个模型, 在目标顶点周围选择最优的邻近顶点进行深层次的多轮对话.  
&emsp;&emsp;低层次的层次策略使用了基于生成模型的多级映射,在用户对话和主题上生成多轮对话.因此提供了一个以目标驱动的生成机制,该机制包括主题选择和使用该主题指导生成.这样可以保证第二个子任务的完成.  

KnowHRL使用advantage actorcritic(A2C)方法去优化long-term developer-defined rewards(**强化学习**)  

&emsp;&emsp;贡献主要有两点:
- 本工作首次尝试将多轮开放领域生成任务划分为两个子任务:目标序列规划 和 通过主题细化来完成目标。根据这一策略，我们提出KnowHRL模型。
- 在知识图谱的帮助下,使得策略学习中的 对话状态和对话行为变得具有可解释性, 他有两个优点: 1、设计一个目标相关的奖励机制去优化目标计划是比较方便的. 2、为了回复具有更好的连贯性和更多的信息量, 使用目标信息去引导回复生成
