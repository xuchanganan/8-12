# 简介
&emsp;&emsp;分类问题中, 通过**改变训练样本的权重**, 学习多个分类器, 并将这些分类器进行线性组合, 提高分类的性能。  
<font color='red'>改变训练样本权重，其实就是在改变样本分布</font>  



# AdaBoost为什么能够提高学习精度?   
见训练误差分析.  
AdaBoost能在学习过程中不断减少训练误差.  
**训练误差分析, 其实也给定了AdaBoost的损失函数是 指数损失.** 

# 解释AdaBoost 
利用**前向分步加法模型**的角度解释.  
**其实就是数学归纳法, 最小化损失函数**  
**因为学习的是加法模型,  如果能够从前到后, 每一步只学习一个基函数及其系数， 逐步逼近优化目标函数式. 就可以简化优化的复杂度**   
分步的意思就是, 每步只学习一个。 

# 提升方法的思想
&emsp;&emsp; 强可学习和弱可学习(可学习，但结果仅比随机猜测略好)是等价的， 寻找弱可学习简单， 解决从弱可学习->强可学习, 因此提出了Boosting.  

# AdaBoost思想  
&emsp;&emsp;Adaboost加大分类误差率小的弱分类器的权值, 使其在表决中起较大作用, 减少分类误差率大的弱分类器的权值.  
- Adaboost提高那些被前一轮弱分类器错误分类样本的权值, 降低那些被正确分类样本的权值. 
- **加权多数表决**
