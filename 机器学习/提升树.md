# 回归树
&emsp;&emsp;回归树可以处理连续变量. 损失函数用的是平方误差损失函数, 每个叶结点是个常数.  
&emsp;&emsp;**回归树是二值划分, 寻找最优划分特征和最优划分点. 划分点就是values, 划分特征就是features, 实现的时候所有样本就某个给定特征的所有取值做set集合，然后依次遍历，<=的放到左边, >的放到右边.**  
&emsp;&emsp;将输入空间划分为J个互不相交的区域, 并且在每个区域上确定输出的常量c

# 模型树
&emsp;&emsp;模型树和回归树最大的区别就是, 模型树的叶子结点是线性模型. 

# 提升树(boosting Tree)
&emsp;&emsp;**提升树是以分类树或回归树为基本分类器的提升方法。**  
&emsp;&emsp;  
&emsp;&emsp;针对不同问题的提升树学习算法， **主要区别在于: 使用的损失函数不同, 包括用平方误差损失函数的回归问题, 用指数损失函数的分类问题， 以及用一般损失函数的一般决策问题。**

# 梯度提升树(GBT)
&emsp;&emsp;提升树当损失函数是平方损失和指数损失函数时, 每一步优化是很简单的。但是对一般函数而言，往往每一步优化并不那么容易。
&emsp;&emsp;关键是**利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中的残差的近似值**，来拟合一个回归树。

# Xgboost
